{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac3f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:18:28.165777Z",
     "iopub.status.busy": "2023-07-08T20:18:28.165328Z",
     "iopub.status.idle": "2023-07-08T20:18:37.730169Z",
     "shell.execute_reply": "2023-07-08T20:18:37.729206Z"
    },
    "papermill": {
     "duration": 9.577312,
     "end_time": "2023-07-08T20:18:37.733345",
     "exception": false,
     "start_time": "2023-07-08T20:18:28.156033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import Levenshtein as lev\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55cb31f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:18:37.769097Z",
     "iopub.status.busy": "2023-07-08T20:18:37.768189Z",
     "iopub.status.idle": "2023-07-08T20:18:37.778159Z",
     "shell.execute_reply": "2023-07-08T20:18:37.776165Z"
    },
    "papermill": {
     "duration": 0.036146,
     "end_time": "2023-07-08T20:18:37.784875",
     "exception": false,
     "start_time": "2023-07-08T20:18:37.748729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = np.random.randint(50000)\n",
    "print(seed)\n",
    "seed = 35567\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa57541a",
   "metadata": {
    "papermill": {
     "duration": 0.010981,
     "end_time": "2023-07-08T20:18:37.808726",
     "exception": false,
     "start_time": "2023-07-08T20:18:37.797745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc499556",
   "metadata": {
    "papermill": {
     "duration": 0.01092,
     "end_time": "2023-07-08T20:18:37.830840",
     "exception": false,
     "start_time": "2023-07-08T20:18:37.819920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Here I use new tokens for padding, start and end of sentences. (Capitals are good since the original phrases have only lower case letters, besides numbers and various signs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a292b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:18:37.855355Z",
     "iopub.status.busy": "2023-07-08T20:18:37.854818Z",
     "iopub.status.idle": "2023-07-08T20:18:37.859985Z",
     "shell.execute_reply": "2023-07-08T20:18:37.859173Z"
    },
    "papermill": {
     "duration": 0.021498,
     "end_time": "2023-07-08T20:18:37.863679",
     "exception": false,
     "start_time": "2023-07-08T20:18:37.842181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad_token = 'P'\n",
    "start_token = 'S'\n",
    "end_token = 'E'\n",
    "pad_token_idx = 59\n",
    "start_token_idx = 60\n",
    "end_token_idx = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f1112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:18:37.887637Z",
     "iopub.status.busy": "2023-07-08T20:18:37.887285Z",
     "iopub.status.idle": "2023-07-08T20:18:38.094923Z",
     "shell.execute_reply": "2023-07-08T20:18:38.094083Z"
    },
    "papermill": {
     "duration": 0.223366,
     "end_time": "2023-07-08T20:18:38.097792",
     "exception": false,
     "start_time": "2023-07-08T20:18:37.874426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    char_to_num = json.load(f)\n",
    "\n",
    "char_to_num[pad_token] = pad_token_idx\n",
    "char_to_num[start_token] = start_token_idx\n",
    "char_to_num[end_token] = end_token_idx\n",
    "num_to_char = {j:i for i,j in char_to_num.items()}\n",
    "\n",
    "inpdir = \"/kaggle/input/asl-fingerspelling\"\n",
    "df = pd.read_csv(f'{inpdir}/train.csv')\n",
    "\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE\n",
    "\n",
    "RHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\n",
    "LHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\n",
    "POSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n",
    "\n",
    "X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\n",
    "Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\n",
    "Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n",
    "\n",
    "SEL_COLS = X + Y + Z\n",
    "FRAME_LEN = 128\n",
    "\n",
    "X_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\n",
    "Y_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\n",
    "Z_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n",
    "\n",
    "RHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\n",
    "LHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\n",
    "RPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\n",
    "LPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a91d8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:18:38.122301Z",
     "iopub.status.busy": "2023-07-08T20:18:38.121616Z",
     "iopub.status.idle": "2023-07-08T20:18:38.151558Z",
     "shell.execute_reply": "2023-07-08T20:18:38.150583Z"
    },
    "papermill": {
     "duration": 0.045594,
     "end_time": "2023-07-08T20:18:38.154673",
     "exception": false,
     "start_time": "2023-07-08T20:18:38.109079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize_pad(x):\n",
    "    if tf.shape(x)[0] < FRAME_LEN:\n",
    "        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n",
    "    else:\n",
    "        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n",
    "    return x\n",
    "\n",
    "def pre_process(x):\n",
    "    rhand = tf.gather(x, RHAND_IDX, axis=1)\n",
    "    lhand = tf.gather(x, LHAND_IDX, axis=1)\n",
    "    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n",
    "    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n",
    "    \n",
    "    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n",
    "    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n",
    "    \n",
    "    rnans = tf.math.count_nonzero(rnan_idx)\n",
    "    lnans = tf.math.count_nonzero(lnan_idx)\n",
    "    \n",
    "    # For dominant hand\n",
    "    if rnans > lnans:\n",
    "        hand = lhand\n",
    "        pose = lpose\n",
    "        \n",
    "        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n",
    "        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n",
    "        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n",
    "        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n",
    "        \n",
    "        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n",
    "        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n",
    "        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n",
    "        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n",
    "    else:\n",
    "        hand = rhand\n",
    "        pose = rpose\n",
    "    \n",
    "    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n",
    "    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n",
    "    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n",
    "    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n",
    "    \n",
    "    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n",
    "    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n",
    "    hand = (hand - mean) / std\n",
    "\n",
    "    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n",
    "    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n",
    "    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n",
    "    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n",
    "    \n",
    "    x = tf.concat([hand, pose], axis=1)\n",
    "    x = resize_pad(x)\n",
    "    \n",
    "    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd1141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:18:38.180314Z",
     "iopub.status.busy": "2023-07-08T20:18:38.179659Z",
     "iopub.status.idle": "2023-07-08T20:18:43.071494Z",
     "shell.execute_reply": "2023-07-08T20:18:43.070423Z"
    },
    "papermill": {
     "duration": 4.908045,
     "end_time": "2023-07-08T20:18:43.074678",
     "exception": false,
     "start_time": "2023-07-08T20:18:38.166633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=list(char_to_num.keys()),\n",
    "        values=list(char_to_num.values()),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"class_weight\"\n",
    ")\n",
    "\n",
    "def preprocess_fn(landmarks, phrase):\n",
    "    phrase = start_token + phrase + end_token\n",
    "    phrase = tf.strings.bytes_split(phrase)\n",
    "    phrase = table.lookup(phrase)\n",
    "    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode = 'CONSTANT',\n",
    "                    constant_values = pad_token_idx)\n",
    "    return pre_process(landmarks), phrase\n",
    "\n",
    "def decode_fn(record_bytes):\n",
    "    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n",
    "    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n",
    "    features = tf.io.parse_single_example(record_bytes, schema)\n",
    "    phrase = features[\"phrase\"]\n",
    "    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in SEL_COLS])\n",
    "    landmarks = tf.transpose(landmarks)\n",
    "    \n",
    "    return landmarks, phrase\n",
    "\n",
    "inpdir = \"/kaggle/input/aslfr-parquets-to-tfrecords-cleaned\"\n",
    "tffiles = df.file_id.map(lambda x: f'{inpdir}/tfds/{x}.tfrecord').unique()\n",
    "\n",
    "batch_size = 32\n",
    "val_len = int(0.05 * len(tffiles))\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(tffiles[val_len:]).map(decode_fn).map(preprocess_fn).shuffle(30000, reshuffle_each_iteration=True).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = tf.data.TFRecordDataset(tffiles[:val_len]).map(decode_fn).map(preprocess_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de772a2",
   "metadata": {
    "papermill": {
     "duration": 0.009631,
     "end_time": "2023-07-08T20:18:43.094137",
     "exception": false,
     "start_time": "2023-07-08T20:18:43.084506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0a3f8",
   "metadata": {
    "papermill": {
     "duration": 0.009056,
     "end_time": "2023-07-08T20:18:43.112730",
     "exception": false,
     "start_time": "2023-07-08T20:18:43.103674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Here I implemented proper positional embeddings for both the encoder and the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42500d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:18:43.133647Z",
     "iopub.status.busy": "2023-07-08T20:18:43.133268Z",
     "iopub.status.idle": "2023-07-08T20:18:43.153814Z",
     "shell.execute_reply": "2023-07-08T20:18:43.152761Z"
    },
    "papermill": {
     "duration": 0.033885,
     "end_time": "2023-07-08T20:18:43.156258",
     "exception": false,
     "start_time": "2023-07-08T20:18:43.122373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
    "        super().__init__()\n",
    "        self.num_hid = num_hid\n",
    "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
    "        #self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "        '''\n",
    "        self.pos_emb = tf.math.divide(\n",
    "            self.positional_encoding(maxlen-1, num_hid),\n",
    "            tf.math.sqrt(tf.cast(num_hid, tf.float32)))\n",
    "        '''\n",
    "        self.pos_emb = self.positional_encoding(maxlen-1, num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        x = self.emb(x)\n",
    "        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n",
    "        '''\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "        '''\n",
    "        return x + self.pos_emb[:maxlen, :]\n",
    "    \n",
    "    def positional_encoding(self, maxlen, num_hid):\n",
    "        depth = num_hid/2\n",
    "        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n",
    "        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n",
    "        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n",
    "        angle_rads = tf.linalg.matmul(positions, angle_rates)\n",
    "        pos_encoding = tf.concat(\n",
    "          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n",
    "          axis=-1) \n",
    "        return pos_encoding\n",
    "\n",
    "\n",
    "class LandmarkEmbedding(layers.Layer):\n",
    "    def __init__(self, num_hid=64, maxlen=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n",
    "        self.maxlen = maxlen\n",
    "        self.num_hid = num_hid\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n",
    "        x = x + self.pos_emb\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def positional_encoding(self, maxlen, num_hid):\n",
    "        depth = num_hid/2\n",
    "        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n",
    "        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n",
    "        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n",
    "        angle_rads = tf.linalg.matmul(positions, angle_rates)\n",
    "        pos_encoding = tf.concat(\n",
    "          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n",
    "          axis=-1) \n",
    "        return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf45f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:18:43.176306Z",
     "iopub.status.busy": "2023-07-08T20:18:43.175569Z",
     "iopub.status.idle": "2023-07-08T20:18:43.185099Z",
     "shell.execute_reply": "2023-07-08T20:18:43.184129Z"
    },
    "papermill": {
     "duration": 0.022547,
     "end_time": "2023-07-08T20:18:43.187475",
     "exception": false,
     "start_time": "2023-07-08T20:18:43.164928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9f8f1",
   "metadata": {
    "papermill": {
     "duration": 0.008893,
     "end_time": "2023-07-08T20:18:43.206085",
     "exception": false,
     "start_time": "2023-07-08T20:18:43.197192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Here I added the training flag to the TransformerDecoder's Dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd16fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:18:43.226928Z",
     "iopub.status.busy": "2023-07-08T20:18:43.226545Z",
     "iopub.status.idle": "2023-07-08T20:18:43.241191Z",
     "shell.execute_reply": "2023-07-08T20:18:43.240174Z"
    },
    "papermill": {
     "duration": 0.028138,
     "end_time": "2023-07-08T20:18:43.243570",
     "exception": false,
     "start_time": "2023-07-08T20:18:43.215432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.self_att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.self_dropout = layers.Dropout(0.5)\n",
    "        self.enc_dropout = layers.Dropout(0.1)\n",
    "        self.ffn_dropout = layers.Dropout(0.1)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
    "\n",
    "        This prevents flow of information from future tokens to current token.\n",
    "        1's in the lower triangle, counting from the lower right corner.\n",
    "        \"\"\"\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, enc_out, target, training):\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
    "        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n",
    "        enc_out = self.enc_att(target_norm, enc_out)\n",
    "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n",
    "        ffn_out = self.ffn(enc_out_norm)\n",
    "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n",
    "        return ffn_out_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf1216",
   "metadata": {
    "papermill": {
     "duration": 0.009145,
     "end_time": "2023-07-08T20:18:43.262275",
     "exception": false,
     "start_time": "2023-07-08T20:18:43.253130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Here I made the passing of the training flag explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e6724e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:18:43.282504Z",
     "iopub.status.busy": "2023-07-08T20:18:43.282141Z",
     "iopub.status.idle": "2023-07-08T20:18:43.307474Z",
     "shell.execute_reply": "2023-07-08T20:18:43.306435Z"
    },
    "papermill": {
     "duration": 0.038405,
     "end_time": "2023-07-08T20:18:43.309799",
     "exception": false,
     "start_time": "2023-07-08T20:18:43.271394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid=64,\n",
    "        num_head=2,\n",
    "        num_feed_forward=128,\n",
    "        source_maxlen=100,\n",
    "        target_maxlen=100,\n",
    "        num_layers_enc=4,\n",
    "        num_layers_dec=1,\n",
    "        num_classes=60,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_input = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder = keras.Sequential(\n",
    "            [self.enc_input]\n",
    "            + [\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i in range(num_layers_dec):\n",
    "            setattr(\n",
    "                self,\n",
    "                f\"dec_layer_{i}\",\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
    "            )\n",
    "\n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "\n",
    "    def decode(self, enc_out, target, training):\n",
    "        y = self.dec_input(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n",
    "        return y\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        source = inputs[0]\n",
    "        target = inputs[1]\n",
    "        x = self.encoder(source, training)\n",
    "        y = self.decode(x, target, training)\n",
    "        return self.classifier(y)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric]\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
    "        source = batch[0]\n",
    "        target = batch[1]\n",
    "\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        \n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self([source, dec_input])\n",
    "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n",
    "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def test_step(self, batch):        \n",
    "        source = batch[0]\n",
    "        target = batch[1]\n",
    "\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        \n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        preds = self([source, dec_input])\n",
    "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "        mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n",
    "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
    "        bs = tf.shape(source)[0]\n",
    "        enc = self.encoder(source, training = False)\n",
    "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
    "        dec_logits = []\n",
    "        for i in range(self.target_maxlen - 1):\n",
    "            dec_out = self.decode(enc, dec_input, training = False)\n",
    "            logits = self.classifier(dec_out)\n",
    "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "            last_logit = logits[:, -1][..., tf.newaxis]\n",
    "            dec_logits.append(last_logit)\n",
    "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
    "        return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d640481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:18:43.330540Z",
     "iopub.status.busy": "2023-07-08T20:18:43.329559Z",
     "iopub.status.idle": "2023-07-08T20:18:45.892560Z",
     "shell.execute_reply": "2023-07-08T20:18:45.891617Z"
    },
    "papermill": {
     "duration": 2.575588,
     "end_time": "2023-07-08T20:18:45.894830",
     "exception": false,
     "start_time": "2023-07-08T20:18:43.319242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(val_dataset))\n",
    "idx_to_char = list(char_to_num.keys())\n",
    "\n",
    "model = Transformer(\n",
    "    num_hid=200,\n",
    "    num_head=4,\n",
    "    num_feed_forward=400,\n",
    "    source_maxlen = FRAME_LEN,\n",
    "    target_maxlen=64,\n",
    "    num_layers_enc=2,\n",
    "    num_layers_dec=1,\n",
    "    num_classes=62,\n",
    ")\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(0.0001)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa1a8c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:18:45.912892Z",
     "iopub.status.busy": "2023-07-08T20:18:45.912589Z",
     "iopub.status.idle": "2023-07-08T20:52:42.348769Z",
     "shell.execute_reply": "2023-07-08T20:52:42.344450Z"
    },
    "papermill": {
     "duration": 2036.455246,
     "end_time": "2023-07-08T20:52:42.358673",
     "exception": false,
     "start_time": "2023-07-08T20:18:45.903427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(train_dataset, verbose = 2, validation_data=val_dataset, epochs=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07dddae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:52:42.381647Z",
     "iopub.status.busy": "2023-07-08T20:52:42.380917Z",
     "iopub.status.idle": "2023-07-08T20:52:42.424424Z",
     "shell.execute_reply": "2023-07-08T20:52:42.423626Z"
    },
    "papermill": {
     "duration": 0.06069,
     "end_time": "2023-07-08T20:52:42.429414",
     "exception": false,
     "start_time": "2023-07-08T20:52:42.368724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec231a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:52:42.453816Z",
     "iopub.status.busy": "2023-07-08T20:52:42.453508Z",
     "iopub.status.idle": "2023-07-08T20:52:42.856078Z",
     "shell.execute_reply": "2023-07-08T20:52:42.855065Z"
    },
    "papermill": {
     "duration": 0.4173,
     "end_time": "2023-07-08T20:52:42.858316",
     "exception": false,
     "start_time": "2023-07-08T20:52:42.441016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce53776",
   "metadata": {
    "papermill": {
     "duration": 0.012,
     "end_time": "2023-07-08T20:52:42.882580",
     "exception": false,
     "start_time": "2023-07-08T20:52:42.870580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88899bb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:52:42.910511Z",
     "iopub.status.busy": "2023-07-08T20:52:42.910147Z",
     "iopub.status.idle": "2023-07-08T20:52:48.836776Z",
     "shell.execute_reply": "2023-07-08T20:52:48.835664Z"
    },
    "papermill": {
     "duration": 5.943567,
     "end_time": "2023-07-08T20:52:48.840273",
     "exception": false,
     "start_time": "2023-07-08T20:52:42.896706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batches = [batch for batch in val_dataset]\n",
    "\n",
    "preds_list = []\n",
    "ground_truth_list = []\n",
    "\n",
    "for batch in batches[:1]:\n",
    "    source = batch[0]\n",
    "    target = batch[1].numpy()\n",
    "    bs = tf.shape(source)[0]\n",
    "    preds = model.generate(source, start_token_idx)\n",
    "    preds = preds.numpy()\n",
    "\n",
    "    for i in range(bs):\n",
    "        target_text = \"\".join([idx_to_char[_] for _ in target[i, :]])\n",
    "        ground_truth_list.append(target_text.replace('P', ''))\n",
    "        prediction = \"\"\n",
    "        for idx in preds[i, :]:\n",
    "            prediction += idx_to_char[idx]\n",
    "            if idx == end_token_idx:\n",
    "                break\n",
    "        preds_list.append(prediction)\n",
    "\n",
    "for i in range(10):\n",
    "    print(ground_truth_list[i])\n",
    "    print(preds_list[i])\n",
    "    print('\\n~~~\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15549e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:52:48.867823Z",
     "iopub.status.busy": "2023-07-08T20:52:48.867443Z",
     "iopub.status.idle": "2023-07-08T20:52:48.883560Z",
     "shell.execute_reply": "2023-07-08T20:52:48.882466Z"
    },
    "papermill": {
     "duration": 0.033786,
     "end_time": "2023-07-08T20:52:48.887062",
     "exception": false,
     "start_time": "2023-07-08T20:52:48.853276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ground_truth_processed = [ground_truth_list[i][1:-1] for i in range(len(ground_truth_list))]\n",
    "preds_list_processed = [preds_list[i][1:-1] for i in range(len(preds_list))]\n",
    "lev_dist = [lev.distance(ground_truth_processed[i], preds_list_processed[i]) \n",
    "            for i in range(len(preds_list_processed))]\n",
    "N = [len(phrase) for phrase in ground_truth_processed]\n",
    "\n",
    "print('Validation score: '+str((np.sum(N) - np.sum(lev_dist))/np.sum(N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c9d05",
   "metadata": {
    "papermill": {
     "duration": 0.012686,
     "end_time": "2023-07-08T20:52:48.912502",
     "exception": false,
     "start_time": "2023-07-08T20:52:48.899816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TFLiteModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae1ee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:52:48.940077Z",
     "iopub.status.busy": "2023-07-08T20:52:48.939075Z",
     "iopub.status.idle": "2023-07-08T20:52:48.950686Z",
     "shell.execute_reply": "2023-07-08T20:52:48.949765Z"
    },
    "papermill": {
     "duration": 0.027813,
     "end_time": "2023-07-08T20:52:48.952879",
     "exception": false,
     "start_time": "2023-07-08T20:52:48.925066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " class TFLiteModel(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TFLiteModel, self).__init__()\n",
    "        self.target_start_token_idx = start_token_idx\n",
    "        self.target_end_token_idx = end_token_idx\n",
    "        # Load the feature generation and main models\n",
    "        self.model = model\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs, training=False):\n",
    "        # Preprocess Data\n",
    "        x = tf.cast(inputs, tf.float32)\n",
    "        x = x[None]\n",
    "        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(SEL_COLS))), lambda: tf.identity(x))\n",
    "        x = x[0]\n",
    "        x = pre_process(x)\n",
    "        x = x[None]\n",
    "        x = self.model.generate(x, self.target_start_token_idx)\n",
    "        x = x[0]\n",
    "        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n",
    "        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n",
    "        x = x[1:idx]\n",
    "        x = tf.one_hot(x, 59)\n",
    "        return {'outputs': x}\n",
    "    \n",
    "tflitemodel_base = TFLiteModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef8102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:52:48.979992Z",
     "iopub.status.busy": "2023-07-08T20:52:48.979017Z",
     "iopub.status.idle": "2023-07-08T20:52:49.131569Z",
     "shell.execute_reply": "2023-07-08T20:52:49.130408Z"
    },
    "papermill": {
     "duration": 0.172007,
     "end_time": "2023-07-08T20:52:49.137374",
     "exception": false,
     "start_time": "2023-07-08T20:52:48.965367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf6419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:52:49.164012Z",
     "iopub.status.busy": "2023-07-08T20:52:49.163646Z",
     "iopub.status.idle": "2023-07-08T20:53:46.330590Z",
     "shell.execute_reply": "2023-07-08T20:53:46.329603Z"
    },
    "papermill": {
     "duration": 57.183235,
     "end_time": "2023-07-08T20:53:46.333202",
     "exception": false,
     "start_time": "2023-07-08T20:52:49.149967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n",
    "keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "tflite_model = keras_model_converter.convert()\n",
    "with open('/kaggle/working/model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "infargs = {\"selected_columns\" : SEL_COLS}\n",
    "\n",
    "with open('inference_args.json', \"w\") as json_file:\n",
    "    json.dump(infargs, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce13ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:53:46.360093Z",
     "iopub.status.busy": "2023-07-08T20:53:46.359766Z",
     "iopub.status.idle": "2023-07-08T20:53:48.833204Z",
     "shell.execute_reply": "2023-07-08T20:53:48.831967Z"
    },
    "papermill": {
     "duration": 2.49026,
     "end_time": "2023-07-08T20:53:48.836342",
     "exception": false,
     "start_time": "2023-07-08T20:53:46.346082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!zip submission.zip  './model.tflite' './inference_args.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9595bd41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-08T20:53:48.876296Z",
     "iopub.status.busy": "2023-07-08T20:53:48.875802Z",
     "iopub.status.idle": "2023-07-08T20:53:49.501068Z",
     "shell.execute_reply": "2023-07-08T20:53:49.499670Z"
    },
    "papermill": {
     "duration": 0.648397,
     "end_time": "2023-07-08T20:53:49.504216",
     "exception": false,
     "start_time": "2023-07-08T20:53:48.855819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(\"model.tflite\")\n",
    "\n",
    "REQUIRED_SIGNATURE = \"serving_default\"\n",
    "REQUIRED_OUTPUT = \"outputs\"\n",
    "\n",
    "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    character_map = json.load(f)\n",
    "rev_character_map = {j:i for i,j in character_map.items()}\n",
    "\n",
    "found_signatures = list(interpreter.get_signature_list().keys())\n",
    "\n",
    "if REQUIRED_SIGNATURE not in found_signatures:\n",
    "    raise KernelEvalException('Required input signature not found.')\n",
    "\n",
    "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "output = prediction_fn(inputs=batch[0][0])\n",
    "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n",
    "print(prediction_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1896f5f",
   "metadata": {
    "papermill": {
     "duration": 0.020637,
     "end_time": "2023-07-08T20:53:49.547068",
     "exception": false,
     "start_time": "2023-07-08T20:53:49.526431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2136.842427,
   "end_time": "2023-07-08T20:53:53.935631",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-08T20:18:17.093204",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
